{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0e3e7a-26a4-42a0-968e-119afcde28b2",
   "metadata": {},
   "source": [
    "### This notebook demonstrates writing video datasets into Tensorflow `tfrecord` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e33d86-7397-4591-b293-568a1e509674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 22:07:10.737227: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-09 22:07:11.273908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:07:11.273973: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-09 22:07:11.372312: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-09 22:07:13.135996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:07:13.136307: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:07:13.136354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63161d04-7758-491e-a9f0-fbd775d590ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory for the video.\n",
    "if not os.path.exists('../inputs/video'):\n",
    "    os.mkdir('../inputs/video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3c54ef-920f-4f34-b01c-4becf0d5d8d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_video = '../inputs/video.mp4'\n",
    "\n",
    "#Read the video with opencv.\n",
    "cap = cv2.VideoCapture(sample_video)\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #Convert image to RGB.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #Get the number of image frames in the video.\n",
    "    image_frame_num = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if not ret:\n",
    "        print('No video frames found')\n",
    "        break\n",
    "        \n",
    "    #Save video frames to images.\n",
    "    filename = f'../inputs/video/{i}.jpg'\n",
    "    cv2.imwrite(filename, frame)\n",
    "    \n",
    "    #Increment i by 1.\n",
    "    i += 1\n",
    "    \n",
    "    if i == image_frame_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4eea433-c2bf-4af0-a271-497c701287ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 22:07:15.810673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:07:15.810754: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-09 22:07:15.810822: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (debonair): /proc/driver/nvidia/version does not exist\n",
      "2022-10-09 22:07:15.811619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-09 22:07:20.225055: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 978739200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Feature.MergeFrom() takes exactly one argument (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m video_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../inputs/video\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msunset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_video_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_string\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m16\u001b[39m]:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line)\n",
      "File \u001b[0;32m~/Documents/Jupyter/Computer Vision/Projects/Video-recognition-for-ASL/notebooks/utils.py:55\u001b[0m, in \u001b[0;36m_parse_video_function\u001b[0;34m(frame_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m image_seq \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(image_seq, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     52\u001b[0m image_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(image_seq)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample(features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Feature.MergeFrom() takes exactly one argument (3 given)"
     ]
    }
   ],
   "source": [
    "video_string = '../inputs/video'\n",
    "label = 'sunset'\n",
    "\n",
    "for line in str(utils._parse_video_function(video_string)).split('\\n')[:16]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20bb23b-6ed4-4282-a70c-1818bca264e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Feature.MergeFrom() takes exactly one argument (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Jupyter/Computer Vision/Projects/Video-recognition-for-ASL/notebooks/utils.py:44\u001b[0m, in \u001b[0;36mvideo_sample\u001b[0;34m(frame_path, label)\u001b[0m\n\u001b[1;32m     34\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frame_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     feature_dict \u001b[38;5;241m=\u001b[39m {image_path: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mstring),\n\u001b[1;32m     37\u001b[0m                     \u001b[38;5;66;03m# 'image_raw': _bytes_feature(image_path),\u001b[39;00m\n\u001b[1;32m     38\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mstring)\n\u001b[1;32m     42\u001b[0m                     }\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample(features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Feature.MergeFrom() takes exactly one argument (3 given)"
     ]
    }
   ],
   "source": [
    "print(utils.video_sample(video_string, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5f9a0-8aeb-4384-a989-9928f6e0a42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
