{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0e3e7a-26a4-42a0-968e-119afcde28b2",
   "metadata": {},
   "source": [
    "### This notebook demonstrates writing video datasets into Tensorflow `tfrecord` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e33d86-7397-4591-b293-568a1e509674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 22:10:36.166724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-09 22:10:36.694510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:10:36.694566: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-09 22:10:36.797841: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-09 22:10:38.835371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:10:38.835637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:10:38.835669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63161d04-7758-491e-a9f0-fbd775d590ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory for the video.\n",
    "if not os.path.exists('../inputs/video'):\n",
    "    os.mkdir('../inputs/video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3c54ef-920f-4f34-b01c-4becf0d5d8d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_video = '../inputs/video.mp4'\n",
    "\n",
    "#Read the video with opencv.\n",
    "cap = cv2.VideoCapture(sample_video)\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #Convert image to RGB.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #Get the number of image frames in the video.\n",
    "    image_frame_num = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if not ret:\n",
    "        print('No video frames found')\n",
    "        break\n",
    "        \n",
    "    #Save video frames to images.\n",
    "    filename = f'../inputs/video/{i}.jpg'\n",
    "    cv2.imwrite(filename, frame)\n",
    "    \n",
    "    #Increment i by 1.\n",
    "    i += 1\n",
    "    \n",
    "    if i == image_frame_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4eea433-c2bf-4af0-a271-497c701287ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 22:10:41.841553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/debonair/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-09 22:10:41.841693: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-09 22:10:41.841775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (debonair): /proc/driver/nvidia/version does not exist\n",
      "2022-10-09 22:10:41.842851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-09 22:10:41.927970: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at example_parsing_ops.cc:94 : INVALID_ARGUMENT: Could not parse example input, value: '../inputs/video/0.jpg'\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ParseExampleV2_Tdense_4_num_sparse_0_ragged_split_types_0_ragged_value_types_0_sparse_types_0_device_/job:localhost/replica:0/task:0/device:CPU:0}} Could not parse example input, value: '../inputs/video/0.jpg' [Op:ParseExampleV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m video_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../inputs/video\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msunset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_video_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_string\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m16\u001b[39m]:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line)\n",
      "File \u001b[0;32m~/Documents/Jupyter/Computer Vision/Projects/Video-recognition-for-ASL/notebooks/utils.py:43\u001b[0m, in \u001b[0;36m_parse_video_function\u001b[0;34m(frame_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frame_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m feature_dict \u001b[38;5;241m=\u001b[39m {image_path : tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mstring),\n\u001b[1;32m     39\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64 ,default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64 ,default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     41\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64 ,default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m                 }\n\u001b[0;32m---> 43\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_single_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m image_buffer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(features[image_path], shape \u001b[38;5;241m=\u001b[39m [])\n\u001b[1;32m     48\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_jpeg(image_buffer, channels \u001b[38;5;241m=\u001b[39m num_depth)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ParseExampleV2_Tdense_4_num_sparse_0_ragged_split_types_0_ragged_value_types_0_sparse_types_0_device_/job:localhost/replica:0/task:0/device:CPU:0}} Could not parse example input, value: '../inputs/video/0.jpg' [Op:ParseExampleV2]"
     ]
    }
   ],
   "source": [
    "video_string = '../inputs/video'\n",
    "label = 'sunset'\n",
    "\n",
    "for line in str(utils._parse_video_function(video_string)).split('\\n')[:16]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20bb23b-6ed4-4282-a70c-1818bca264e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Feature.MergeFrom() takes exactly one argument (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Jupyter/Computer Vision/Projects/Video-recognition-for-ASL/notebooks/utils.py:44\u001b[0m, in \u001b[0;36mvideo_sample\u001b[0;34m(frame_path, label)\u001b[0m\n\u001b[1;32m     34\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frame_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     feature_dict \u001b[38;5;241m=\u001b[39m {image_path: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mstring),\n\u001b[1;32m     37\u001b[0m                     \u001b[38;5;66;03m# 'image_raw': _bytes_feature(image_path),\u001b[39;00m\n\u001b[1;32m     38\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mstring)\n\u001b[1;32m     42\u001b[0m                     }\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample(features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Feature.MergeFrom() takes exactly one argument (3 given)"
     ]
    }
   ],
   "source": [
    "print(utils.video_sample(video_string, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5f9a0-8aeb-4384-a989-9928f6e0a42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
